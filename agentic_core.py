
import os
from openai import OpenAI
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Initialize the client only once in the file
client = OpenAI()

def get_initial_claims(paper_text):
    """Generates initial claims from each persona based on the paper's text
    """

    # defining the personas as a dictionary for easy iteration
    personas = {
       "Optimist": ("You are the Optimist. You see the potential and positive "
                    "implications of new scinetific findings. Identify the most "
                    "promising claims and explain their significance. Be sure to "
                    "cite specific sentences or sections from the paper."),

       "Skeptic": ("You are the Skeptic. You are a critical peer reviewer who "
                   "questions everything. Identify potential weaknesses, limitations, "
                   "or unsubstantiated claims in the paper. Explain your reasoning "
                   "and cite specific sentences or sections."),

        "Ethicist": ("You are the Ethicist. You are an expert in AI ethics. Identify "
                     "any potential ethical implications, both positive and negative, "
                     "including data usage, bias, or societal impact. Explain your "
                     "concerns and cite specific sentences or sections.")
    }

    # A chunk of the paper's text to fit within the context window
    text_chunk = paper_text[:8000]

    claims = {}

    # Iterate through each personas and get their initial analysis
    for persona, prompt_text in personas.items():
        print(f"\n--- Generating claims for {persona} ---")

        try:
            # OpenAI uses a message-based chat completion API
            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": prompt_text},
                    {"role": "user", "content": f"Based on the following paper text, provide a detailed analysis:\n\n{text_chunk}"}    
                ]
            )
            # Access the generated content from the response object
            claims[persona] = response.choices[0].message.content
        except Exception as e:
            claims[persona] = f"Error generating content for {persona}: {e}"
            print(claims[persona])

    return claims

if __name__ == "__main__":
    print("This script is ready to be integrated with the PDF ingestion module.")



def run_debate(initial_claims, paper_text, max_rounds=3):
    """
    Simulates a multi-round debate between the personas.
    
    Args:
        initial_claims (dict): The initial claims generated by each persona.
        paper_text (str): The full text of the paper for context.
        max_rounds (int): The maximum number of debate rounds to simulate.

    Returns:
        list: A list of debate rounds, where each round contains the claims and counter-claims.

    """
    debate_history = []

    # Start the debate with the initial claims
    debate_history.append({"round": 0, "Optimist": initial_claims["Optimist"], "Skeptic": initial_claims["Skeptic"], "Ethicist": initial_claims["Ethicist"]})

    # A chunk of the paper's text to fit within the context window
    text_chunk = paper_text[:8000]

    # Iterate through each round of the debate
    for i in range(1, max_rounds + 1):
        print(f"\n--- Starting Debate Round {i} ---")

        previous_optimist_claim = debate_history[-1].get("Optimist", "")
        previous_skpetic_claim = debate_history[-1].get("Skeptic", "")
        previous_ethicist_claim = debate_history[-1].get("Ethicist", "")

        current_round = {}

        # 1. Skeptic responds to the Optimist's previous claim
        skeptic_prompt = (
            f"You are the Skeptic. The Optimist has made the following claim: '{previous_optimist_claim}'. "
            f"Provide a counter-argument or a point of nuance, ciitng the paper to suggest your position. "
            f"Here is the paper text for reference:\n\n{text_chunk}"
        )
        skeptic_response = client.chat.completions.create(
            model="gpt4o-mini",
            messages=[{"role": "user", "content": skeptic_prompt}]
        ).choices[0].message.content
        current_round["Skeptic"] = skeptic_response

        # 2. Optimist responds to the Skeptic's new claim
        optimist_prompt = (
            f"You are the Optimist. The Skeptic has just made the following counter-claim: '{current_round['Skeptic']}'. "
            f"Provide a rebuttal or a different perspective that supports the paper's findings. "
            f"Here is the paper text for reference:\n\n{text_chunk}"
        )
        optimist_response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": optimist_prompt}]
        ).choices[0].message.content
        current_round["Optimist"] = optimist_response

        # 3. Ethicist responds to the debate between the other two
        ethicist_prompt = (
            f"You are the Ethicist. The Optimist and Skeptic are debating. The Optimist's last point was: '{current_round['Optimist']}'. "
            f"The Skeptic's last pint was: '{current_round['Skeptic']}'. "
            f"What are the ethical implications of their debate? Add a new ethical claim or concern, citing the paper. "
            f"Here is the paper text for reference:\n\n{text_chunk}"
        )
        ethicist_response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": ethicist_prompt}]
        ).choices[0].message.content

        current_round["round"] = i
        debate_history.append(current_round)

    return debate_history


                                  
